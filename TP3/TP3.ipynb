{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE-411, HomeWork 3 : Neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Backpropagation with logistic loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = 5\n",
    "K = 6\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def predict(X, W):\n",
    "    # X: B x D\n",
    "    # W: {w1: D x K, w2: K x 1}\n",
    "    # z1: B x K\n",
    "    # z2: B x 1\n",
    "    # yhat: B x 1\n",
    "    z1 = np.dot(X, W['w1'])\n",
    "    x1 = sigmoid(z1)\n",
    "    z2 = np.dot(x1, W['w2'])\n",
    "    yhat = sigmoid(z2)\n",
    "    return z1, z2, yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)Logistic Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def logistic_loss(y, yhat):\n",
    "    # y: B x 1\n",
    "    # yhat: B x 1\n",
    "    # loss: B x 1\n",
    "    B = y.shape[0]\n",
    "    loss_sum = np.sum(-y * np.log(yhat) - (1 - y) * np.log(1 - yhat)) / B\n",
    "    return loss_sum\n",
    "\n",
    "\n",
    "# testing with yhat =  y -> 0, not possible to compute log(0)\n",
    "y = np.ones(10)    * 0.000000000000000\n",
    "yhat = np.ones(10) * 0.000000000000000001\n",
    "print(logistic_loss(y, yhat))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\mathbf y \\simeq \\mathbf{\\^{y}} \\simeq 0$, we get an average logistic loss of nearly 0 for the whole batch. This is expected, as the expected value and the ground truth are equal. However, $0$ is not a valid value for this is undetermined for the $log(\\mathbf{\\^{y}})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)Stable Logistic Loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a stable logistic loss function, we'll employ $z_2$ and the activation function (sigmoid in this case), instead of directly using $\\^y$. By injecting :\n",
    "\n",
    "$$\\^y = \\sigma(z_2) = \\frac{1}{1 + e^{-z_2}}$$\n",
    "\n",
    "Into:\n",
    "\n",
    "$$ \\mathcal{L} = -y\\cdot log\\left(\\^y\\right) -(1-y)\\cdot log\\left(1 - \\^y\\right)$$\n",
    "\n",
    "We get:\n",
    "$$ \\mathcal{L} = -y\\cdot log\\left( \\frac{1}{1 + e^{-z_2}}\\right) -(1-y)\\cdot log\\left(\\frac{e^{-z_2}}{1 + e^{-z_2}}\\right)$$\n",
    "\n",
    "With basic manipulations, we get the following final expression for the logistic loss function:\n",
    "\n",
    "$$ \\mathcal{L} = -z_2\\cdot y + log(e^{-z_2} + 1)$$\n",
    "\n",
    "Which does not have the same issue as the normal logistic function used above, as we have a stable function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For z_2 = -10E10, y = 0\n",
      "Normal Logistic loss function : nan\n",
      "Stable Logistic loss function : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osour\\AppData\\Local\\Temp\\ipykernel_7256\\3499368462.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\osour\\AppData\\Local\\Temp\\ipykernel_7256\\539335239.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_sum = np.sum(-y * np.log(yhat) - (1 - y) * np.log(1 - yhat)) / B\n",
      "C:\\Users\\osour\\AppData\\Local\\Temp\\ipykernel_7256\\539335239.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss_sum = np.sum(-y * np.log(yhat) - (1 - y) * np.log(1 - yhat)) / B\n"
     ]
    }
   ],
   "source": [
    "def stable_logistic_loss(y, z2):\n",
    "    # y: B x 1\n",
    "    # z2: B x 1\n",
    "    # loss: B x 1\n",
    "    B = y.shape[0]\n",
    "    loss_sum = np.sum(-y * z2 + np.logaddexp(0, z2))/B\n",
    "    return loss_sum\n",
    "\n",
    "\n",
    "z_2 = -10E10 * np.ones(10)\n",
    "y = 0 * np.ones(10)\n",
    "print(\"For z_2 = -10E10, y = 0\")\n",
    "print(\"Normal Logistic loss function :\", logistic_loss(y, sigmoid(z_2)))\n",
    "print(\"Stable Logistic loss function :\", stable_logistic_loss(y, z_2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Partial Derivatives of the loss with respect to the weights\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is, simply put, a method of calculating partial derivatives by working backwards, from the output to the intermediary results, through the weights all up to the input. The chain rule is incredibly useful for this. Let's start with:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_i} \\mathcal L(\\vec x, y, \\vec w) == \\frac{\\partial\\mathcal L}{\\partial \\^y} \\cdot \\frac{\\partial \\^y}{\\partial w_i^{(2)}}$$\n",
    "\n",
    "$$ \\rightarrow \\frac{\\partial \\^y}{\\partial w_i^{(2)}} = \\frac{\\partial}{\\partial w_i^{(2)}} \\left(\\frac{1}{1 + e^{-\\vec w_2^T \\cdot \\vec x^{(1)}}}\\right) = \\frac{\\partial}{\\partial w_i^{(2)}} \\left(\\frac{1}{1 + e^{- w_{2,i} \\cdot  x_i^{(1)}}}\\right) = \\frac{x_i^{(1)\\cdot e^{-w_i^{(2)}x_i} }}{\\left( 1 + e^{-w_i^{(2)}x_i} \\right)^2 }$$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal L}{\\partial \\^y}  = \\frac{\\partial}{\\partial \\^y} \\left( -y\\cdot log(\\^y) -(1 - y)\\cdot log(1-\\^y) \\right) = \\frac{\\^y -y}{y\\cdot (1-\\^y)}$$\n",
    "\n",
    "Which gives us:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_i^{(2)}} \\mathcal L(\\vec x, y, \\vec w) = \\frac{\\partial \\mathcal L}{\\partial \\^y} \\cdot \\frac{\\partial \\^y}{\\partial w_i^{(2)}} = \\frac{\\^y -y}{y\\cdot (1-\\^y)} \\cdot \\frac{x_i^{(1)\\cdot e^{-w_i^{(2)}x_i} }}{\\left( 1 + e^{-w_i^{(2)}x_i} \\right)^2 }$$\n",
    "\n",
    "With the same logic, we can get the partial derivative of the loss with respect to the weights of the first layer:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_i^{(1)}} \\mathcal L(\\vec x, y, \\vec w) = \\frac{\\partial \\mathcal L}{\\partial \\^y} \\cdot \\frac{\\partial \\^y}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_i^{(1)}}$$\n",
    "\n",
    "$$ \\rightarrow \\frac{\\partial \\^y}{\\partial z_2} = \\frac{\\partial}{\\partial z_2} \\left(\\frac{1}{1 + e^{-\\vec w_2^T \\cdot \\vec x^{(1)}}}\\right) = \\frac{\\partial}{\\partial z_2} \\left(\\frac{1}{1 + e^{- \\vec w_2^T \\cdot \\vec x^{(1)}}}\\right) = \\frac{e^{-z_2}}{\\left( 1 + e^{-z_2} \\right)^2 }$$\n",
    "\n",
    "$$ \\frac{\\partial z_2}{\\partial w_i^{(1)}} = \\frac{\\partial}{\\partial w_i^{(1)}} \\left(\\vec w_2^T \\cdot \\vec x^{(1)}\\right) = \\frac{\\partial}{\\partial w_i^{(1)}} \\left(w_{2,i} \\cdot  x_i^{(1)}\\right) = x_i^{(1)}$$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal L}{\\partial \\^y}  = \\frac{\\partial}{\\partial \\^y} \\left( -y\\cdot log(\\^y) -(1 - y)\\cdot log(1-\\^y) \\right) = \\frac{\\^y -y}{y\\cdot (1-\\^y)}$$\n",
    "\n",
    "Which gives us:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_i^{(1)}} \\mathcal L(\\vec x, y, \\vec w) = \\frac{\\partial \\mathcal L}{\\partial \\^y} \\cdot \\frac{\\partial \\^y}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_i^{(1)}} = \\frac{\\^y -y}{y\\cdot (1-\\^y)} \\cdot \\frac{e^{-z_2}}{\\left( 1 + e^{-z_2} \\right)^2 } \\cdot x_i^{(1)}$$\n",
    "\n",
    "\n",
    "With $\\vec x^{(1)}$ being the input vector, $\\vec w_2$ being the weights of the second layer, and $\\vec w_1$ being the weights of the first layer. By replacing:\n",
    "\n",
    "$$ \\^y = \\sigma \\left( \\vec w^{(2)T}\\cdot \\sigma \\left( \\vec w^{(1)T}\\cdot \\vec x^{(0)} \\right) \\right)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$ z^{(2)} = \\vec w^{(2)T}\\cdot \\vec x^{(1)}$$ \n",
    "\n",
    "Where $\\sigma(z)$ is the sigmoid function:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Gradient Descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss_grad(X, y, W):\n",
    "    # X: B x D\n",
    "    # y: B x 1\n",
    "    # W: {w1: D x K, w2: K x 1}\n",
    "    # z1: B x K\n",
    "    # z2: B x 1\n",
    "    # yhat: B x 1\n",
    "    # grad: {w1: D x K, w2: K x 1}\n",
    "    B = X.shape[0]\n",
    "    z1, z2, yhat = predict(X, W)\n",
    "    grad = {}\n",
    "    grad['w2'] = np.dot(sigmoid(z1).T, yhat - y) / B\n",
    "    grad['w1'] = np.dot(X.T, np.dot(yhat - y, W['w2'].T) * sigmoid(z1) * (1 - sigmoid(z1))) / B\n",
    "    return grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classifying FashionMNIST using neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load dataset and construct dataloader "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is given in the PIL (*Python Image Library*) format. We therefore need to convert it to a type readable by the Neural Network, which is why we use *ToTensor()* transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# load the train dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform)\n",
    "\n",
    "# load the test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data/', \n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform)\n",
    "# hyperparameters\n",
    "# 50000 images for training\n",
    "# 100 epochs\n",
    "# 10000 images for testing\n",
    "BATCH_SIZE = 2500\n",
    "TEST_BATCH_SIZE = 10000\n",
    "num_epochs = 20\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    num_workers=2)\n",
    "\n",
    "\n",
    "# Construct the dataloader for the testing dataset.\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Multilayer Perceptron \n",
    "\n",
    "The MLP will be a simple two hidden layer, with 100 neurons per layer, rectified linear units as activation functions, and a linear output layer. 20 epochs are used for training, using the cross-entropy loss and the following optimizers:\n",
    "1. SGD with learning rate 0.01\n",
    "2. SGD with momentum 0.9, learning rate 0.01 and nesterov momentum\n",
    "3. Adam with learning rate 0.01\n",
    "4. Adam with learning rate 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the exercice sessions, we'll define the following functions:\n",
    "\n",
    "1. *train()* for training the model\n",
    "2. *test()* for testing the model\n",
    "3. *plot()* for plotting the results\n",
    "4. *predict()* for predicting the class of a given image, and prints the percentage of confidence of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0002, Accuracy: 963/10000 (10%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00023059289455413818, 9.63)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # fc : fully connected, 28*28 = 784 because the images are 28x28\n",
    "        # input layer\n",
    "        self.fc0 = nn.Linear(784, 392)\n",
    "        # first hidden layer\n",
    "        self.fc1 = nn.Linear(392, 196)\n",
    "        # second hidden layer \n",
    "        self.fc2 = nn.Linear(196, 98)\n",
    "        # output layer\n",
    "        self.fc3 = nn.Linear(98, 10)\n",
    "        # activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        # transform the image into a vector\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        # forward pass through the layers\n",
    "        x = self.fc0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_epoch(\n",
    "    model : nn.Module,\n",
    "    train_loader : DataLoader,\n",
    "    optimizer : torch.optim,\n",
    "    device : torch.device,\n",
    "    epoch : int\n",
    "    ):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "        # move data and target to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # compute the loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        # print statistics information\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader.dataset)\n",
    "\n",
    "def fit(\n",
    "    model : nn.Module,\n",
    "    train_loader : DataLoader,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    device : torch.device\n",
    "    ):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        current_loss = train_epoch( model,\n",
    "                                    train_loader, \n",
    "                                    optimizer, \n",
    "                                    device, \n",
    "                                    epoch)\n",
    "        print(f\"Epoch {epoch} loss: {current_loss}\")\n",
    "        losses.append(current_loss)\n",
    "    return losses\n",
    "\n",
    "def predict(\n",
    "            model : nn.Module,\n",
    "            test_loader : DataLoader,\n",
    "            device : torch.device\n",
    "            ):\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\")\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = neural_network()\n",
    "# move the model to the GPU/CPU according to availability\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# create optimizers\n",
    "SGD_optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "SGD_momentum_optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "Adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "Adam2_optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "\n",
    "# sanity check \n",
    "predict(model, test_dataloader, DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up and we performed a sanity check, we can train the 20 epochs for each optimizer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.0009215566754341125\n",
      "Epoch 1 loss: 0.0009208874424298604\n",
      "Epoch 2 loss: 0.0009202144225438436\n",
      "Epoch 3 loss: 0.0009195283571879069\n",
      "Epoch 4 loss: 0.0009188210566838582\n",
      "Epoch 5 loss: 0.0009180858413378398\n",
      "Epoch 6 loss: 0.0009173190315564473\n",
      "Epoch 7 loss: 0.0009165201385815938\n",
      "Epoch 8 loss: 0.0009156840244928996\n",
      "Epoch 9 loss: 0.0009148061037063599\n",
      "Epoch 10 loss: 0.0009138805270195007\n",
      "Epoch 11 loss: 0.0009129003524780274\n",
      "Epoch 12 loss: 0.0009118587096532186\n",
      "Epoch 13 loss: 0.0009107450604438782\n",
      "Epoch 14 loss: 0.0009095487435658773\n",
      "Epoch 15 loss: 0.0009082566618919373\n",
      "Epoch 16 loss: 0.0009068536599477132\n",
      "Epoch 17 loss: 0.0009053213198979696\n",
      "Epoch 18 loss: 0.0009036383231480916\n",
      "Epoch 19 loss: 0.0009017776052157084\n",
      "Epoch 0 loss: 0.0008939918359120686\n",
      "Epoch 1 loss: 0.0008557843724886576\n",
      "Epoch 2 loss: 0.0007341987351576488\n",
      "Epoch 3 loss: 0.0004932477126518885\n",
      "Epoch 4 loss: 0.00031903284788131715\n",
      "Epoch 5 loss: 0.00025518782834211987\n",
      "Epoch 6 loss: 0.0002224420815706253\n",
      "Epoch 7 loss: 0.00020090082635482152\n",
      "Epoch 8 loss: 0.00018519443968931834\n",
      "Epoch 9 loss: 0.0001728969603776932\n",
      "Epoch 10 loss: 0.00016312110473712285\n",
      "Epoch 11 loss: 0.0001550931895772616\n",
      "Epoch 12 loss: 0.00014855449944734575\n",
      "Epoch 13 loss: 0.00014289582669734955\n",
      "Epoch 14 loss: 0.00013796037683884302\n",
      "Epoch 15 loss: 0.0001335941289861997\n",
      "Epoch 16 loss: 0.00012949091990788777\n",
      "Epoch 17 loss: 0.00012570572843154272\n",
      "Epoch 18 loss: 0.00012219723016023635\n",
      "Epoch 19 loss: 0.0001187887986501058\n",
      "Epoch 0 loss: 0.0011819177423914274\n",
      "Epoch 1 loss: 0.00012914643536011378\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "SGD_losses = fit(model, train_dataloader, SGD_optimizer, num_epochs, DEVICE)\n",
    "SGD_momentum_losses = fit(model, train_dataloader, SGD_momentum_optimizer, num_epochs, DEVICE)\n",
    "Adam_losses = fit(model, train_dataloader, Adam_optimizer, num_epochs, DEVICE)\n",
    "Adam2_losses = fit(model, train_dataloader, Adam2_optimizer, num_epochs, DEVICE)\n",
    "\n",
    "# test the model\n",
    "predict(model, test_dataloader, DEVICE)\n",
    "\n",
    "# plot the losses\n",
    "plt.plot(SGD_losses, label=\"SGD\")\n",
    "plt.plot(SGD_momentum_losses, label=\"SGD with momentum\")\n",
    "plt.plot(Adam_losses, label=\"Adam\")\n",
    "plt.plot(Adam2_losses, label=\"Adam2\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Convolution Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f9566ee846e0e5475f3731207e71ee4a96d604221359f666805a9fa43f54da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
